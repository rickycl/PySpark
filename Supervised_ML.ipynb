{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOynfzjqpmIUnS+ruYRqVKy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rickycl/PySpark/blob/master/Supervised_ML.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KryVpPzNxB6C",
        "outputId": "679bc373-3637-465e-e557-fe24196fd0b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.0.tar.gz (316.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.9/316.9 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.0-py2.py3-none-any.whl size=317425345 sha256=527f8cd79c3cd60b89359bbc1d80749c1a6b1a302c3e46b505cb9ae177987e08\n",
            "  Stored in directory: /root/.cache/pip/wheels/41/4e/10/c2cf2467f71c678cfc8a6b9ac9241e5e44a01940da8fbb17fc\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.0\n"
          ]
        }
      ],
      "source": [
        "# Step 1: Create the Spark Session Object\n",
        "!pip install pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "spark=SparkSession.builder.appName('Linear RM').getOrCreate()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In supervised ML, the correct labels/output is already known during the model training phase - the error can be reduced accordingly.\n",
        "\n",
        "Mapping of the relationship between the input data and output label to pick up the signals from the training data and generalize about the unseen data.\n",
        "\n",
        "The training of the model consists of comparing the actual output with the predicted output and then making the changes in predictions, to reduce the total error between what is actual and what is predicted."
      ],
      "metadata": {
        "id": "vM1Q9PyGqSdt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Step 2: Read the Dataset\n",
        "day: last contact day of the month (numeric)\n",
        "\n",
        "duration: last contact duration, in seconds (numeric)\n",
        "\n",
        "campaign: number of contacts performed during this campaign and for this client (numeric, includes last contact)\n",
        "\n",
        "pdays: number of days that passed by after the client was last contacted from a previous campaign (numeric, -1 means client was not previously contacted)\n",
        "\n",
        "previous: number of contacts performed before this campaign and for this client (numeric)\n",
        "\n",
        "target_class: has the client subscribed a term deposit? (binary: \"yes\": 1, \"no\": 0)"
      ],
      "metadata": {
        "id": "Xt-0-1snJaIQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df=spark.read.csv('sample_data/bank-full1.csv', inferSchema=True, header=True)"
      ],
      "metadata": {
        "id": "BTgGwMESxHrT"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print((df.count(), len(df.columns)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "548mioRsxHyx",
        "outputId": "106aa018-7968-4003-cf20-d1b761e81266"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(45211, 8)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eB82djf8xH1g",
        "outputId": "0ff570cb-b023-493c-b549-0b821e53c0c4"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- age: integer (nullable = true)\n",
            " |-- balance: integer (nullable = true)\n",
            " |-- day: integer (nullable = true)\n",
            " |-- duration: integer (nullable = true)\n",
            " |-- campaign: integer (nullable = true)\n",
            " |-- pdays: integer (nullable = true)\n",
            " |-- previous: integer (nullable = true)\n",
            " |-- target_class: integer (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.show(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ir7U5gOnxH9C",
        "outputId": "20ea3177-1ebb-45d8-b7a5-4078803311b9"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-------+---+--------+--------+-----+--------+------------+\n",
            "|age|balance|day|duration|campaign|pdays|previous|target_class|\n",
            "+---+-------+---+--------+--------+-----+--------+------------+\n",
            "| 58|   2143|  5|     261|       1|   -1|       0|           0|\n",
            "| 44|     29|  5|     151|       1|   -1|       0|           0|\n",
            "| 33|      2|  5|      76|       1|   -1|       0|           0|\n",
            "| 47|   1506|  5|      92|       1|   -1|       0|           0|\n",
            "| 33|      1|  5|     198|       1|   -1|       0|           0|\n",
            "| 35|    231|  5|     139|       1|   -1|       0|           0|\n",
            "| 28|    447|  5|     217|       1|   -1|       0|           0|\n",
            "| 42|      2|  5|     380|       1|   -1|       0|           0|\n",
            "| 58|    121|  5|      50|       1|   -1|       0|           0|\n",
            "| 43|    593|  5|      55|       1|   -1|       0|           0|\n",
            "+---+-------+---+--------+--------+-----+--------+------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 3: Feature Engineering\n",
        "Create a single vector combining all input features, by using Spark's VectorAssembler. It creates only a single feature that captures the input values for that particular row.\n",
        "So, instead of 7 input columns, the engine essentially translates the features into a single column with 7 input values, in the form of a list."
      ],
      "metadata": {
        "id": "k6LDOk7Ln3LU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.linalg import Vector\n",
        "from pyspark.ml.feature import VectorAssembler"
      ],
      "metadata": {
        "id": "HH6Kr__yxIAB"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d29A0LGOxICw",
        "outputId": "71c5cc3a-13a4-48f4-b7dd-9d4d643251d9"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['age',\n",
              " 'balance',\n",
              " 'day',\n",
              " 'duration',\n",
              " 'campaign',\n",
              " 'pdays',\n",
              " 'previous',\n",
              " 'target_class']"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vec_assembler=VectorAssembler(inputCols=['age', 'balance', 'day', 'duration', 'campaign', 'pdays', 'previous'], outputCol='features')\n",
        "features_df=vec_assembler.transform(df)\n",
        "features_df.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kp2lJhHRxIFg",
        "outputId": "9ca24e51-4bdb-4359-ccb5-b352d33135d7"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- age: integer (nullable = true)\n",
            " |-- balance: integer (nullable = true)\n",
            " |-- day: integer (nullable = true)\n",
            " |-- duration: integer (nullable = true)\n",
            " |-- campaign: integer (nullable = true)\n",
            " |-- pdays: integer (nullable = true)\n",
            " |-- previous: integer (nullable = true)\n",
            " |-- target_class: integer (nullable = true)\n",
            " |-- features: vector (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Extra column (features) contains the single dense vector for all of the inputs\n",
        "# The column target_class had to be renamed\n",
        "features_df = features_df.withColumnRenamed('target_class', 'label')"
      ],
      "metadata": {
        "id": "MVTHXzCv6YHN"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features_df.select(['features', 'label']).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IP2IvBqjxIIY",
        "outputId": "b65be3e2-7752-4ae4-ccb7-e1265d1ae718"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+-----+\n",
            "|            features|label|\n",
            "+--------------------+-----+\n",
            "|[58.0,2143.0,5.0,...|    0|\n",
            "|[44.0,29.0,5.0,15...|    0|\n",
            "|[33.0,2.0,5.0,76....|    0|\n",
            "|[47.0,1506.0,5.0,...|    0|\n",
            "|[33.0,1.0,5.0,198...|    0|\n",
            "|[35.0,231.0,5.0,1...|    0|\n",
            "|[28.0,447.0,5.0,2...|    0|\n",
            "|[42.0,2.0,5.0,380...|    0|\n",
            "|[58.0,121.0,5.0,5...|    0|\n",
            "|[43.0,593.0,5.0,5...|    0|\n",
            "|[41.0,270.0,5.0,2...|    0|\n",
            "|[29.0,390.0,5.0,1...|    0|\n",
            "|[53.0,6.0,5.0,517...|    0|\n",
            "|[58.0,71.0,5.0,71...|    0|\n",
            "|[57.0,162.0,5.0,1...|    0|\n",
            "|[51.0,229.0,5.0,3...|    0|\n",
            "|[45.0,13.0,5.0,98...|    0|\n",
            "|[57.0,52.0,5.0,38...|    0|\n",
            "|[60.0,60.0,5.0,21...|    0|\n",
            "|[33.0,0.0,5.0,54....|    0|\n",
            "+--------------------+-----+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Split the Dataset\n",
        "train, test = features_df.randomSplit([0.75, 0.25])\n",
        "print(f\"Size of train Dataset : {train.count()}\" )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9g8_0efWxILY",
        "outputId": "ddf706e8-79f8-4870-84eb-885df41323ec"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size of train Dataset : 33869\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Size of test Dataset : {test.count()}\" )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S6GZ-zLtxIN4",
        "outputId": "8eb95339-ebbb-45a1-b13e-2270f82c47e2"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size of test Dataset : 11342\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5: Build and train Linear RM using features, input and label columns\n",
        "from pyspark.ml.regression import LinearRegression\n",
        "lr = LinearRegression()"
      ],
      "metadata": {
        "id": "4BefgRFRxIQ0"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lr_model = lr.fit(train)\n",
        "predictions_df = lr_model.transform(test)\n",
        "predictions_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xv6gfivexITg",
        "outputId": "68ea4046-489d-4343-c240-7df4375ba5f5"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-------+---+--------+--------+-----+--------+-----+--------------------+--------------------+\n",
            "|age|balance|day|duration|campaign|pdays|previous|label|            features|          prediction|\n",
            "+---+-------+---+--------+--------+-----+--------+-----+--------------------+--------------------+\n",
            "| 18|      5| 24|     143|       2|   -1|       0|    0|[18.0,5.0,24.0,14...|0.027535736957257355|\n",
            "| 18|   1944| 10|     122|       3|   -1|       0|    0|[18.0,1944.0,10.0...|0.020649151558401004|\n",
            "| 19|     56| 12|     246|       1|   -1|       0|    0|[19.0,56.0,12.0,2...| 0.08114475783154632|\n",
            "| 19|     60| 14|     253|       1|   -1|       0|    0|[19.0,60.0,14.0,2...| 0.08468098940621466|\n",
            "| 19|    179| 24|      62|       3|   -1|       0|    0|[19.0,179.0,24.0,...|-0.01371466916044...|\n",
            "| 19|    245| 10|      98|       2|  110|       2|    0|[19.0,245.0,10.0,...| 0.04880677594105844|\n",
            "| 19|    526|  1|     174|       1|  199|       3|    0|[19.0,526.0,1.0,1...|  0.1189909483478403|\n",
            "| 19|    527|  4|     154|       3|   -1|       0|    0|[19.0,527.0,4.0,1...|  0.0313597510857181|\n",
            "| 19|   1803| 10|      59|       1|   -1|       0|    0|[19.0,1803.0,10.0...|-0.00362213484673...|\n",
            "| 19|   1803| 23|     124|       1|  105|       1|    0|[19.0,1803.0,23.0...|0.062407452120760244|\n",
            "| 20|      0|  1|     143|       5|   91|       8|    0|[20.0,0.0,1.0,143...| 0.10092464017110456|\n",
            "| 20|      0|  2|      69|       4|  182|       4|    0|[20.0,0.0,2.0,69....| 0.06052187696726658|\n",
            "| 20|     76| 18|     639|       2|   -1|       0|    1|[20.0,76.0,18.0,6...|   0.270759484831074|\n",
            "| 20|    129| 13|     190|       1|   -1|       0|    0|[20.0,129.0,13.0,...| 0.05488720318035752|\n",
            "| 20|    162| 25|     106|       2|   -1|       0|    0|[20.0,162.0,25.0,...| 0.01157643805926712|\n",
            "| 20|    179| 23|     317|       1|  182|       6|    0|[20.0,179.0,23.0,...|  0.2075992547148022|\n",
            "| 20|    215| 24|     175|       1|   92|       6|    1|[20.0,215.0,24.0,...| 0.11635103442095984|\n",
            "| 20|    291| 11|     172|       5|  371|       5|    0|[20.0,291.0,11.0,...| 0.16329619846034202|\n",
            "| 20|    292|  4|      45|       1|   -1|       0|    0|[20.0,292.0,4.0,4...|-0.01569124022170...|\n",
            "| 20|    502| 30|     261|       1|   -1|       0|    1|[20.0,502.0,30.0,...| 0.09181752229561452|\n",
            "+---+-------+---+--------+--------+-----+--------+-----+--------------------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 6: Evaluate Linear RM on Test Data\n",
        "# To check the performance of the model on unseen or test data\n",
        "model_predictions=lr_model.evaluate(test)\n",
        "model_predictions.r2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ti2nai8KxIWH",
        "outputId": "f8409466-0b3a-4f6e-8365-50b9a49ffa9a"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.179991807283855"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "R squared is the square of the correlation. It measures the proportion of variation in the dependent variable that can be attributed to the independent variable.\n",
        "\n",
        "It mainly suggests how much of the variation in the dataset can be attributed to regression. The higher the value, the better the performance of the model.\n",
        "\n",
        "From the above result, it means that the model is weak because 17.1% is the strength and character of the relationship between the term deposit(dependent variable) and a series of other independent variables(i.e, age, balance, day, duration, duration, campaign, pdays and previous)"
      ],
      "metadata": {
        "id": "sA0r1TkJQTW-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(model_predictions.meanSquaredError)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6YWXijU7xIYv",
        "outputId": "789f0418-c2e5-48d2-b51b-ad194e3eac84"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.08449346613707671\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5iQWyyCWxIcI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#GENERALIZED LINEAR MODEL REGRESSION\n",
        "Target variable has an error distribution other than a preferred normal distribution\n",
        "\n"
      ],
      "metadata": {
        "id": "a7XIu28K0Z6d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.regression import GeneralizedLinearRegression"
      ],
      "metadata": {
        "id": "Yk5uT-rW7vFU"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "glr = GeneralizedLinearRegression()\n",
        "glr_model = glr.fit(train)\n",
        "glr_model.coefficients\n",
        "\n",
        "# One of the features has a negative coefficient value."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J6cfKp-17vIN",
        "outputId": "1166eab7-b673-42af-af61-50838d5fa762"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DenseVector([0.0007, 0.0, 0.0001, 0.0005, -0.0031, 0.0002, 0.0075])"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "glr_model.summary"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1n5ZDx_38Fas",
        "outputId": "567ea570-faa8-4e94-8f97-e6498ed2c952"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Coefficients:\n",
              "    Feature Estimate Std Error T Value P Value\n",
              "(Intercept)  -0.0500    0.0074 -6.7265  0.0000\n",
              "        age   0.0007    0.0002  4.7764  0.0000\n",
              "    balance   0.0000    0.0000  7.2152  0.0000\n",
              "        day   0.0001    0.0002  0.2781  0.7810\n",
              "   duration   0.0005    0.0000 78.2733  0.0000\n",
              "   campaign  -0.0031    0.0005 -5.9528  0.0000\n",
              "      pdays   0.0002    0.0000 13.8317  0.0000\n",
              "   previous   0.0075    0.0007 10.1601  0.0000\n",
              "\n",
              "(Dispersion parameter for gaussian family taken to be 0.0858)\n",
              "    Null deviance: 3501.5884 on 33861 degrees of freedom\n",
              "Residual deviance: 2906.4149 on 33861 degrees of freedom\n",
              "AIC: 12966.0259"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the Model Performance on Test Data\n",
        "model_predictions = glr_model.evaluate(test)\n",
        "model_predictions.predictions.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r8b5ZAFu8Fdk",
        "outputId": "cd678f2b-9b03-427e-eedb-4dc539525a4b"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-------+---+--------+--------+-----+--------+-----+--------------------+--------------------+\n",
            "|age|balance|day|duration|campaign|pdays|previous|label|            features|          prediction|\n",
            "+---+-------+---+--------+--------+-----+--------+-----+--------------------+--------------------+\n",
            "| 18|      5| 24|     143|       2|   -1|       0|    0|[18.0,5.0,24.0,14...|0.027535736957257355|\n",
            "| 18|   1944| 10|     122|       3|   -1|       0|    0|[18.0,1944.0,10.0...|0.020649151558401004|\n",
            "| 19|     56| 12|     246|       1|   -1|       0|    0|[19.0,56.0,12.0,2...| 0.08114475783154632|\n",
            "| 19|     60| 14|     253|       1|   -1|       0|    0|[19.0,60.0,14.0,2...| 0.08468098940621466|\n",
            "| 19|    179| 24|      62|       3|   -1|       0|    0|[19.0,179.0,24.0,...|-0.01371466916044...|\n",
            "| 19|    245| 10|      98|       2|  110|       2|    0|[19.0,245.0,10.0,...| 0.04880677594105844|\n",
            "| 19|    526|  1|     174|       1|  199|       3|    0|[19.0,526.0,1.0,1...|  0.1189909483478403|\n",
            "| 19|    527|  4|     154|       3|   -1|       0|    0|[19.0,527.0,4.0,1...|  0.0313597510857181|\n",
            "| 19|   1803| 10|      59|       1|   -1|       0|    0|[19.0,1803.0,10.0...|-0.00362213484673...|\n",
            "| 19|   1803| 23|     124|       1|  105|       1|    0|[19.0,1803.0,23.0...|0.062407452120760244|\n",
            "| 20|      0|  1|     143|       5|   91|       8|    0|[20.0,0.0,1.0,143...| 0.10092464017110456|\n",
            "| 20|      0|  2|      69|       4|  182|       4|    0|[20.0,0.0,2.0,69....| 0.06052187696726658|\n",
            "| 20|     76| 18|     639|       2|   -1|       0|    1|[20.0,76.0,18.0,6...|   0.270759484831074|\n",
            "| 20|    129| 13|     190|       1|   -1|       0|    0|[20.0,129.0,13.0,...| 0.05488720318035752|\n",
            "| 20|    162| 25|     106|       2|   -1|       0|    0|[20.0,162.0,25.0,...| 0.01157643805926712|\n",
            "| 20|    179| 23|     317|       1|  182|       6|    0|[20.0,179.0,23.0,...|  0.2075992547148022|\n",
            "| 20|    215| 24|     175|       1|   92|       6|    1|[20.0,215.0,24.0,...| 0.11635103442095984|\n",
            "| 20|    291| 11|     172|       5|  371|       5|    0|[20.0,291.0,11.0,...| 0.16329619846034202|\n",
            "| 20|    292|  4|      45|       1|   -1|       0|    0|[20.0,292.0,4.0,4...|-0.01569124022170...|\n",
            "| 20|    502| 30|     261|       1|   -1|       0|    1|[20.0,502.0,30.0,...| 0.09181752229561452|\n",
            "+---+-------+---+--------+--------+-----+--------+-----+--------------------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The Akaike information criterion is an evaluation parameter of relative performance of quality of models for the same set dataset.\n",
        "# It is used to select among multiple models for a given dataset. A lesser value indicates the model is of good quality.\n",
        "# AIC tries to strike a balance between the variance and bias of the model. The model with the lowest AIC is preferred over other models.\n",
        "model_predictions.aic"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_CXtMCl98FiE",
        "outputId": "a8293738-aa0b-485a-ed51-d355da26b053"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4178.200175053163"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "glr = GeneralizedLinearRegression(family='Binomial')\n",
        "glr_model = glr.fit(train)\n",
        "model_predictions=glr_model.evaluate(test)\n",
        "model_predictions.aic"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i2eSugwr8Fku",
        "outputId": "ad6d4dc3-4432-4f7b-c7e6-1867dcc32d97"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6586.788935961635"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "glr = GeneralizedLinearRegression(family='Poisson')\n",
        "glr_model = glr.fit(train)\n",
        "model_predictions=glr_model.evaluate(test)\n",
        "model_predictions.aic\n",
        "\n",
        "# The default GLM model with Gaussian distribution has the lowest AIC value at 4178.2002"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nao6pBFa8rj-",
        "outputId": "253ddcd1-4ad7-4586-cb12-c650fec44d94"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7440.304523979164"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "glr = GeneralizedLinearRegression(family='Gamma')\n",
        "glr_model = glr.fit(train)\n",
        "model_predictions = glr_model.evaluate(test)\n",
        "model_predictions.aic"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Nk52QUec8rm_",
        "outputId": "f62feb0a-0eec-412b-a594-9d334b3780cc"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "error",
          "ename": "Py4JJavaError",
          "evalue": "An error occurred while calling o326.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 50.0 failed 1 times, most recent failure: Lost task 0.0 in stage 50.0 (TID 43) (69ed30299582 executor driver): java.lang.IllegalArgumentException: requirement failed: The response variable of Gamma family should be positive, but got 0.0\n\tat scala.Predef$.require(Predef.scala:281)\n\tat org.apache.spark.ml.regression.GeneralizedLinearRegression$Gamma$.initialize(GeneralizedLinearRegression.scala:823)\n\tat org.apache.spark.ml.regression.GeneralizedLinearRegression$FamilyAndLink.$anonfun$initialize$1(GeneralizedLinearRegression.scala:510)\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)\n\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)\n\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.aggregate(TraversableOnce.scala:260)\n\tat scala.collection.TraversableOnce.aggregate$(TraversableOnce.scala:260)\n\tat scala.collection.AbstractIterator.aggregate(Iterator.scala:1431)\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$4(RDD.scala:1261)\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$6(RDD.scala:1262)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:855)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:855)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:984)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2493)\n\tat org.apache.spark.rdd.RDD.$anonfun$fold$1(RDD.scala:1199)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:407)\n\tat org.apache.spark.rdd.RDD.fold(RDD.scala:1193)\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$2(RDD.scala:1286)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:407)\n\tat org.apache.spark.rdd.RDD.treeAggregate(RDD.scala:1253)\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$1(RDD.scala:1239)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:407)\n\tat org.apache.spark.rdd.RDD.treeAggregate(RDD.scala:1239)\n\tat org.apache.spark.ml.optim.WeightedLeastSquares.fit(WeightedLeastSquares.scala:107)\n\tat org.apache.spark.ml.regression.GeneralizedLinearRegression$FamilyAndLink.initialize(GeneralizedLinearRegression.scala:517)\n\tat org.apache.spark.ml.regression.GeneralizedLinearRegression.$anonfun$train$1(GeneralizedLinearRegression.scala:431)\n\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n\tat org.apache.spark.ml.regression.GeneralizedLinearRegression.train(GeneralizedLinearRegression.scala:380)\n\tat org.apache.spark.ml.regression.GeneralizedLinearRegression.train(GeneralizedLinearRegression.scala:247)\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:114)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: java.lang.IllegalArgumentException: requirement failed: The response variable of Gamma family should be positive, but got 0.0\n\tat scala.Predef$.require(Predef.scala:281)\n\tat org.apache.spark.ml.regression.GeneralizedLinearRegression$Gamma$.initialize(GeneralizedLinearRegression.scala:823)\n\tat org.apache.spark.ml.regression.GeneralizedLinearRegression$FamilyAndLink.$anonfun$initialize$1(GeneralizedLinearRegression.scala:510)\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)\n\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)\n\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.aggregate(TraversableOnce.scala:260)\n\tat scala.collection.TraversableOnce.aggregate$(TraversableOnce.scala:260)\n\tat scala.collection.AbstractIterator.aggregate(Iterator.scala:1431)\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$4(RDD.scala:1261)\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$6(RDD.scala:1262)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:855)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:855)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-9d4cfefe8b83>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mglr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGeneralizedLinearRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfamily\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Gamma'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mglr_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mglr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mmodel_predictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mglr_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmodel_predictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    203\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m             raise TypeError(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    379\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mJM\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 381\u001b[0;31m         \u001b[0mjava_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    382\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjava_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_copyValues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    376\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    377\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 378\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    379\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mJM\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
            "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o326.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 50.0 failed 1 times, most recent failure: Lost task 0.0 in stage 50.0 (TID 43) (69ed30299582 executor driver): java.lang.IllegalArgumentException: requirement failed: The response variable of Gamma family should be positive, but got 0.0\n\tat scala.Predef$.require(Predef.scala:281)\n\tat org.apache.spark.ml.regression.GeneralizedLinearRegression$Gamma$.initialize(GeneralizedLinearRegression.scala:823)\n\tat org.apache.spark.ml.regression.GeneralizedLinearRegression$FamilyAndLink.$anonfun$initialize$1(GeneralizedLinearRegression.scala:510)\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)\n\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)\n\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.aggregate(TraversableOnce.scala:260)\n\tat scala.collection.TraversableOnce.aggregate$(TraversableOnce.scala:260)\n\tat scala.collection.AbstractIterator.aggregate(Iterator.scala:1431)\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$4(RDD.scala:1261)\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$6(RDD.scala:1262)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:855)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:855)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:984)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2493)\n\tat org.apache.spark.rdd.RDD.$anonfun$fold$1(RDD.scala:1199)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:407)\n\tat org.apache.spark.rdd.RDD.fold(RDD.scala:1193)\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$2(RDD.scala:1286)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:407)\n\tat org.apache.spark.rdd.RDD.treeAggregate(RDD.scala:1253)\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$1(RDD.scala:1239)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:407)\n\tat org.apache.spark.rdd.RDD.treeAggregate(RDD.scala:1239)\n\tat org.apache.spark.ml.optim.WeightedLeastSquares.fit(WeightedLeastSquares.scala:107)\n\tat org.apache.spark.ml.regression.GeneralizedLinearRegression$FamilyAndLink.initialize(GeneralizedLinearRegression.scala:517)\n\tat org.apache.spark.ml.regression.GeneralizedLinearRegression.$anonfun$train$1(GeneralizedLinearRegression.scala:431)\n\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n\tat org.apache.spark.ml.regression.GeneralizedLinearRegression.train(GeneralizedLinearRegression.scala:380)\n\tat org.apache.spark.ml.regression.GeneralizedLinearRegression.train(GeneralizedLinearRegression.scala:247)\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:114)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: java.lang.IllegalArgumentException: requirement failed: The response variable of Gamma family should be positive, but got 0.0\n\tat scala.Predef$.require(Predef.scala:281)\n\tat org.apache.spark.ml.regression.GeneralizedLinearRegression$Gamma$.initialize(GeneralizedLinearRegression.scala:823)\n\tat org.apache.spark.ml.regression.GeneralizedLinearRegression$FamilyAndLink.$anonfun$initialize$1(GeneralizedLinearRegression.scala:510)\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)\n\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)\n\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.aggregate(TraversableOnce.scala:260)\n\tat scala.collection.TraversableOnce.aggregate$(TraversableOnce.scala:260)\n\tat scala.collection.AbstractIterator.aggregate(Iterator.scala:1431)\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$4(RDD.scala:1261)\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$6(RDD.scala:1262)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:855)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:855)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DECISION TREE REGRESSION\n",
        "It can be used for both regression and classification. Quite poweful in terms of fitting the data well but comes with the high risk of sometimes overfitting the data. DT contain multiple splits based on entropy or Gini indexes. The deeper the tree, the higher the chance of overfitting the data\n"
      ],
      "metadata": {
        "id": "hHU9cxRv2l21"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build and Train DT Regressor Model\n",
        "from pyspark.ml.regression import DecisionTreeRegressor\n",
        "dec_tree = DecisionTreeRegressor()\n",
        "dec_tree_model = dec_tree.fit(train)\n",
        "dec_tree_model.featureImportances"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HRbNasKR9AOV",
        "outputId": "7f16e326-2042-4bca-a19f-ce40cc619bc9"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SparseVector(7, {0: 0.0685, 1: 0.0033, 2: 0.0304, 3: 0.7019, 4: 0.0012, 5: 0.1947})"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the Model Performance on Test Data\n",
        "model_predictions = dec_tree_model.transform(test)\n",
        "model_predictions.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "unxJiHkS9ARL",
        "outputId": "8f185135-72b3-4192-c927-fe84a73de603"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-------+---+--------+--------+-----+--------+-----+--------------------+--------------------+\n",
            "|age|balance|day|duration|campaign|pdays|previous|label|            features|          prediction|\n",
            "+---+-------+---+--------+--------+-----+--------+-----+--------------------+--------------------+\n",
            "| 18|      5| 24|     143|       2|   -1|       0|    0|[18.0,5.0,24.0,14...|  0.0635048231511254|\n",
            "| 18|   1944| 10|     122|       3|   -1|       0|    0|[18.0,1944.0,10.0...|  0.0635048231511254|\n",
            "| 19|     56| 12|     246|       1|   -1|       0|    0|[19.0,56.0,12.0,2...| 0.15303430079155672|\n",
            "| 19|     60| 14|     253|       1|   -1|       0|    0|[19.0,60.0,14.0,2...| 0.15303430079155672|\n",
            "| 19|    179| 24|      62|       3|   -1|       0|    0|[19.0,179.0,24.0,...|  0.0635048231511254|\n",
            "| 19|    245| 10|      98|       2|  110|       2|    0|[19.0,245.0,10.0,...| 0.07184241019698726|\n",
            "| 19|    526|  1|     174|       1|  199|       3|    0|[19.0,526.0,1.0,1...| 0.15313028764805414|\n",
            "| 19|    527|  4|     154|       3|   -1|       0|    0|[19.0,527.0,4.0,1...|  0.0635048231511254|\n",
            "| 19|   1803| 10|      59|       1|   -1|       0|    0|[19.0,1803.0,10.0...|  0.0635048231511254|\n",
            "| 19|   1803| 23|     124|       1|  105|       1|    0|[19.0,1803.0,23.0...| 0.07184241019698726|\n",
            "| 20|      0|  1|     143|       5|   91|       8|    0|[20.0,0.0,1.0,143...|  0.2692307692307692|\n",
            "| 20|      0|  2|      69|       4|  182|       4|    0|[20.0,0.0,2.0,69....|0.012684989429175475|\n",
            "| 20|     76| 18|     639|       2|   -1|       0|    1|[20.0,76.0,18.0,6...|  0.3130128956623681|\n",
            "| 20|    129| 13|     190|       1|   -1|       0|    0|[20.0,129.0,13.0,...|  0.0635048231511254|\n",
            "| 20|    162| 25|     106|       2|   -1|       0|    0|[20.0,162.0,25.0,...|  0.0635048231511254|\n",
            "| 20|    179| 23|     317|       1|  182|       6|    0|[20.0,179.0,23.0,...| 0.34527687296416937|\n",
            "| 20|    215| 24|     175|       1|   92|       6|    1|[20.0,215.0,24.0,...| 0.34527687296416937|\n",
            "| 20|    291| 11|     172|       5|  371|       5|    0|[20.0,291.0,11.0,...| 0.15313028764805414|\n",
            "| 20|    292|  4|      45|       1|   -1|       0|    0|[20.0,292.0,4.0,4...|  0.0635048231511254|\n",
            "| 20|    502| 30|     261|       1|   -1|       0|    1|[20.0,502.0,30.0,...| 0.15303430079155672|\n",
            "+---+-------+---+--------+--------+-----+--------+-----+--------------------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# To evaluate the performance of the decision tree on test data\n",
        "# r^2 mainly suggests how much of the variation in the dataset can be attributed to regression. The higher the value, the better the performance of the model.\n",
        "# RMSE suggests the total errors the model is making, in terms of the difference between actual and predicted values\n",
        "\n",
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "dt_evaluator = RegressionEvaluator(metricName='r2')\n",
        "dt_r2 = dt_evaluator.evaluate(model_predictions)\n",
        "print(f'The r-square value of DecisionTreeRegressor is {dt_r2}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DLyoCFW09YtY",
        "outputId": "444079bb-3a6b-426f-c31e-b9d3cce9675a"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The r-square value of DecisionTreeRegressor is 0.2337287726638555\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dt_evaluator = RegressionEvaluator(metricName='rmse')\n",
        "dt_rmse = dt_evaluator.evaluate(model_predictions)\n",
        "print(f'The rmse value of DecisionTreeRegressor is {dt_rmse}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TAaEjLF-9Yv8",
        "outputId": "dfd3b33d-a296-41f0-c154-48fd0982835e"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The rmse value of DecisionTreeRegressor is 0.28099185200234106\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#RANDOM FOREST REGRESSORS\n",
        "are a collection of multiple individual decision trees built using different samples of data. A RF is an ensembling technique that takes a bagging approach that can be used for regression and classification. DT tend to overfit the data. RF remove the element of high variance, by taking the means of the predicted values from individual trees."
      ],
      "metadata": {
        "id": "BWCqxYgp4_W5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build and Train RF Regressor Model\n",
        "from pyspark.ml.regression import RandomForestRegressor\n",
        "rf = RandomForestRegressor()\n",
        "rf_model = rf.fit(train)\n",
        "rf_model.featureImportances"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7JZeZBuR97KQ",
        "outputId": "8d5dc5a8-24cd-428f-dd5d-c0a7859e58f3"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SparseVector(7, {0: 0.0759, 1: 0.016, 2: 0.0237, 3: 0.6807, 4: 0.006, 5: 0.1388, 6: 0.0588})"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rf_model.getNumTrees"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LKW_hAnV97Og",
        "outputId": "284c5272-e69a-479b-f8bd-cbd2a2c08023"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "20"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_predictions = rf_model.transform(test)"
      ],
      "metadata": {
        "id": "P96QXGSB-Il_"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_predictions.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Sy27FpK-Io7",
        "outputId": "43bdfe7d-2229-41cb-d40a-aeb67a645fef"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-------+---+--------+--------+-----+--------+-----+--------------------+-------------------+\n",
            "|age|balance|day|duration|campaign|pdays|previous|label|            features|         prediction|\n",
            "+---+-------+---+--------+--------+-----+--------+-----+--------------------+-------------------+\n",
            "| 18|      5| 24|     143|       2|   -1|       0|    0|[18.0,5.0,24.0,14...|0.05442794375135808|\n",
            "| 18|   1944| 10|     122|       3|   -1|       0|    0|[18.0,1944.0,10.0...|0.06002948475348282|\n",
            "| 19|     56| 12|     246|       1|   -1|       0|    0|[19.0,56.0,12.0,2...|0.08550211821875787|\n",
            "| 19|     60| 14|     253|       1|   -1|       0|    0|[19.0,60.0,14.0,2...|0.08550211821875787|\n",
            "| 19|    179| 24|      62|       3|   -1|       0|    0|[19.0,179.0,24.0,...|0.05923398029888029|\n",
            "| 19|    245| 10|      98|       2|  110|       2|    0|[19.0,245.0,10.0,...|0.15429846250237636|\n",
            "| 19|    526|  1|     174|       1|  199|       3|    0|[19.0,526.0,1.0,1...|0.13941208047440132|\n",
            "| 19|    527|  4|     154|       3|   -1|       0|    0|[19.0,527.0,4.0,1...|0.06687534798490288|\n",
            "| 19|   1803| 10|      59|       1|   -1|       0|    0|[19.0,1803.0,10.0...|0.07776841589566216|\n",
            "| 19|   1803| 23|     124|       1|  105|       1|    0|[19.0,1803.0,23.0...| 0.1663720105319022|\n",
            "| 20|      0|  1|     143|       5|   91|       8|    0|[20.0,0.0,1.0,143...|0.28884158743862126|\n",
            "| 20|      0|  2|      69|       4|  182|       4|    0|[20.0,0.0,2.0,69....|0.07724298098264962|\n",
            "| 20|     76| 18|     639|       2|   -1|       0|    1|[20.0,76.0,18.0,6...|  0.301317063920533|\n",
            "| 20|    129| 13|     190|       1|   -1|       0|    0|[20.0,129.0,13.0,...|0.07921346124336541|\n",
            "| 20|    162| 25|     106|       2|   -1|       0|    0|[20.0,162.0,25.0,...|0.06817974558161513|\n",
            "| 20|    179| 23|     317|       1|  182|       6|    0|[20.0,179.0,23.0,...|  0.446098756886931|\n",
            "| 20|    215| 24|     175|       1|   92|       6|    1|[20.0,215.0,24.0,...| 0.4285883030841027|\n",
            "| 20|    291| 11|     172|       5|  371|       5|    0|[20.0,291.0,11.0,...|0.14269468685785616|\n",
            "| 20|    292|  4|      45|       1|   -1|       0|    0|[20.0,292.0,4.0,4...| 0.0693208621168691|\n",
            "| 20|    502| 30|     261|       1|   -1|       0|    1|[20.0,502.0,30.0,...|0.11306255964421505|\n",
            "+---+-------+---+--------+--------+-----+--------+-----+--------------------+-------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the Model Performance on Test Data\n",
        "rf_evaluator = RegressionEvaluator(metricName='r2')\n",
        "rf_r2 = rf_evaluator.evaluate(model_predictions)\n",
        "print(f'The r-square value of RandomForestRegressor is {rf_r2}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5xr2QR3X-Iry",
        "outputId": "7ccfbb69-ad13-4a92-e5a8-0692e1bb6bdf"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The r-square value of RandomForestRegressor is 0.24921720756668175\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rf_evaluator = RegressionEvaluator(metricName='rmse')"
      ],
      "metadata": {
        "id": "R9k1OF1s-awo"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rf_rmse = rf_evaluator.evaluate(model_predictions)"
      ],
      "metadata": {
        "id": "j9bokTJY-azo"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "PAvK7Pgn7zjM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'The rmse value of RandomForestRegressor is {rf_rmse}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r0nJafWf-a2I",
        "outputId": "2168eb5c-a574-45ea-cc99-b2db896e0579"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The rmse value of RandomForestRegressor is 0.27813754856576156\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "SIU16pMb1OAv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#GRADIENT-BOOSTED TREE REGRESSOR\n",
        "is also an ensembling technique, which uses boosting, i.e making use of individual weak learners in order to boost the performance of the overall model.\n",
        "In bagging, the individual models that are built are parallel in nature, meaning they can be built independent of each other.\n",
        "In boosting, the individual models are built in a sequential manner. In a gradient boosting approach, the second model focuses on the errors made by the first model and tries to reduce overall errors for those data points. Similarly, the next model tries to reduce the errors made by the previous model. In this way, the overall error of prediction is reduced."
      ],
      "metadata": {
        "id": "_-txdSze7JQm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build and Train the GBT Regressor Model\n",
        "from pyspark.ml.regression import GBTRegressor\n",
        "gbt = GBTRegressor()\n",
        "gbt_model=gbt.fit(train)\n",
        "gbt_model.featureImportances"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1bq__LnT-7oJ",
        "outputId": "4b54f528-04d7-475e-c79f-4f2642c1466c"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SparseVector(7, {0: 0.1275, 1: 0.076, 2: 0.1628, 3: 0.3579, 4: 0.0449, 5: 0.1968, 6: 0.0341})"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_predictions = gbt_model.transform(test)\n",
        "model_predictions.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QRJqkXGX-7rQ",
        "outputId": "78250193-fe32-4979-d493-b7bd69cdda73"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-------+---+--------+--------+-----+--------+-----+--------------------+--------------------+\n",
            "|age|balance|day|duration|campaign|pdays|previous|label|            features|          prediction|\n",
            "+---+-------+---+--------+--------+-----+--------+-----+--------------------+--------------------+\n",
            "| 18|      5| 24|     143|       2|   -1|       0|    0|[18.0,5.0,24.0,14...| 0.08665512988784398|\n",
            "| 18|   1944| 10|     122|       3|   -1|       0|    0|[18.0,1944.0,10.0...| 0.19009065676638015|\n",
            "| 19|     56| 12|     246|       1|   -1|       0|    0|[19.0,56.0,12.0,2...| 0.15322133431935805|\n",
            "| 19|     60| 14|     253|       1|   -1|       0|    0|[19.0,60.0,14.0,2...|  0.1521330255411778|\n",
            "| 19|    179| 24|      62|       3|   -1|       0|    0|[19.0,179.0,24.0,...|0.025074852259006786|\n",
            "| 19|    245| 10|      98|       2|  110|       2|    0|[19.0,245.0,10.0,...|  0.3264474782807645|\n",
            "| 19|    526|  1|     174|       1|  199|       3|    0|[19.0,526.0,1.0,1...| 0.27506016811956324|\n",
            "| 19|    527|  4|     154|       3|   -1|       0|    0|[19.0,527.0,4.0,1...| 0.11999835667562486|\n",
            "| 19|   1803| 10|      59|       1|   -1|       0|    0|[19.0,1803.0,10.0...| 0.07584820539223885|\n",
            "| 19|   1803| 23|     124|       1|  105|       1|    0|[19.0,1803.0,23.0...|  0.2540844177668406|\n",
            "| 20|      0|  1|     143|       5|   91|       8|    0|[20.0,0.0,1.0,143...|  0.4127314150247953|\n",
            "| 20|      0|  2|      69|       4|  182|       4|    0|[20.0,0.0,2.0,69....|-0.03816056026454...|\n",
            "| 20|     76| 18|     639|       2|   -1|       0|    1|[20.0,76.0,18.0,6...|  0.3103294471051348|\n",
            "| 20|    129| 13|     190|       1|   -1|       0|    0|[20.0,129.0,13.0,...|  0.1864398367547462|\n",
            "| 20|    162| 25|     106|       2|   -1|       0|    0|[20.0,162.0,25.0,...| 0.14460296871326206|\n",
            "| 20|    179| 23|     317|       1|  182|       6|    0|[20.0,179.0,23.0,...|  0.7518156780510787|\n",
            "| 20|    215| 24|     175|       1|   92|       6|    1|[20.0,215.0,24.0,...|  0.6271573476567581|\n",
            "| 20|    291| 11|     172|       5|  371|       5|    0|[20.0,291.0,11.0,...| 0.31327821778069576|\n",
            "| 20|    292|  4|      45|       1|   -1|       0|    0|[20.0,292.0,4.0,4...|0.010736409691407343|\n",
            "| 20|    502| 30|     261|       1|   -1|       0|    1|[20.0,502.0,30.0,...| 0.34921585830486057|\n",
            "+---+-------+---+--------+--------+-----+--------+-----+--------------------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the Model Performance on Test Data\n",
        "gbt_evaluator = RegressionEvaluator(metricName='r2')\n",
        "gbt_r2 = gbt_evaluator.evaluate(model_predictions)\n",
        "print(f'The r-square value of GradientBoostedRegressor is {gbt_r2}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r_lnb6hp_NNw",
        "outputId": "5e2a5c7c-c094-46ba-ab45-e0163f0a3134"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The r-square value of GradientBoostedRegressor is 0.25712331871354677\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gbt_evaluator = RegressionEvaluator(metricName='rmse')\n",
        "gbt_rmse = gbt_evaluator.evaluate(model_predictions)\n",
        "print(f'The rmse value of GradientBoostedRegressor is {gbt_rmse}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nnx6Lgn-_UG4",
        "outputId": "470b9ad6-71c6-4999-9590-96a855ceba88"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The rmse value of GradientBoostedRegressor is 0.2766692103328004\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The GBT regressor outperforms the random forest model with both the values of r-square and rmse being higher at (25.71, 27.67) against (24.92, 27.81) for the random forest model."
      ],
      "metadata": {
        "id": "XrSBULvs_qZe"
      }
    }
  ]
}